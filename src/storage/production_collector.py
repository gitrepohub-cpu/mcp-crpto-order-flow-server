"""
ğŸš€ 24/7 Production Data Collector
=================================
Collects data from ALL exchanges for ALL coins with STRICT spot/futures separation.
Each stream type has its own exact column schema matching the database tables.

Features:
- 9 symbols Ã— 9 exchanges Ã— 10 data streams = 527 total feeds
- Market type separation (futures/spot/oracle) in EVERY record
- Buffered writes for efficiency (configurable batch size)
- Auto-reconnection on disconnect
- Hourly database checkpoints
- Daily Backblaze B2 backup (optional)
"""

import asyncio
import duckdb
import json
import os
import sys
import logging
from datetime import datetime, timezone, timedelta
from pathlib import Path
from collections import defaultdict
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Tuple
import time
import traceback

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('logs/collector.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SYMBOLS = [
    'BTCUSDT', 'ETHUSDT', 'SOLUSDT', 'XRPUSDT', 'ARUSDT',  # Major
    'BRETTUSDT', 'POPCATUSDT', 'WIFUSDT', 'PNUTUSDT'        # Meme
]

# Exchange configurations with WebSocket endpoints
EXCHANGES = {
    # FUTURES EXCHANGES
    'binance_futures': {
        'type': 'futures',
        'ws_url': 'wss://fstream.binance.com/stream',
        'all_symbols': True,  # All 9 symbols available
        'streams': ['price', 'orderbook', 'trade', 'mark_price', 'funding_rate', 
                   'open_interest', 'ticker_24h', 'candle', 'liquidation']
    },
    'bybit_futures': {
        'type': 'futures',
        'ws_url': 'wss://stream.bybit.com/v5/public/linear',
        'excluded_symbols': ['WIFUSDT'],  # WIFUSDT not on Bybit futures
        'streams': ['price', 'orderbook', 'trade', 'mark_price', 'funding_rate',
                   'open_interest', 'ticker_24h', 'candle', 'liquidation']
    },
    'okx_futures': {
        'type': 'futures',
        'ws_url': 'wss://ws.okx.com:8443/ws/v5/public',
        'excluded_symbols': ['PNUTUSDT'],  # PNUTUSDT not on OKX
        'streams': ['price', 'orderbook', 'trade', 'mark_price', 'funding_rate',
                   'open_interest', 'ticker_24h', 'candle', 'liquidation']
    },
    'kraken_futures': {
        'type': 'futures',
        'ws_url': 'wss://futures.kraken.com/ws/v1',
        'excluded_symbols': ['BRETTUSDT', 'ARUSDT'],  # Not on Kraken
        'streams': ['price', 'orderbook', 'trade', 'mark_price', 'funding_rate',
                   'open_interest', 'ticker_24h', 'candle']  # No liquidation
    },
    'gateio_futures': {
        'type': 'futures',
        'ws_url': 'wss://fx-ws.gateio.ws/v4/ws/usdt',
        'all_symbols': True,
        'streams': ['price', 'orderbook', 'trade', 'mark_price', 'funding_rate',
                   'open_interest', 'ticker_24h', 'candle', 'liquidation']
    },
    'hyperliquid': {
        'type': 'futures',
        'ws_url': 'wss://api.hyperliquid.xyz/ws',
        'excluded_symbols': ['BRETTUSDT', 'ARUSDT'],
        'streams': ['price', 'orderbook', 'trade', 'mark_price', 'funding_rate',
                   'open_interest', 'candle', 'liquidation']  # No ticker_24h
    },
    # SPOT EXCHANGES
    'binance_spot': {
        'type': 'spot',
        'ws_url': 'wss://stream.binance.com:9443/stream',
        'symbols': ['BTCUSDT', 'ETHUSDT', 'SOLUSDT', 'XRPUSDT', 'ARUSDT', 
                   'WIFUSDT', 'PNUTUSDT'],
        'streams': ['price', 'orderbook', 'trade', 'ticker_24h', 'candle']
    },
    'bybit_spot': {
        'type': 'spot',
        'ws_url': 'wss://stream.bybit.com/v5/public/spot',
        'all_symbols': True,
        'streams': ['price', 'orderbook', 'trade', 'ticker_24h', 'candle']
    },
    # ORACLE
    'pyth_oracle': {
        'type': 'oracle',
        'ws_url': 'wss://hermes.pyth.network/ws',
        'symbols': ['BTCUSDT', 'ETHUSDT', 'SOLUSDT', 'XRPUSDT', 'ARUSDT'],
        'streams': ['price']  # Oracle only provides price
    }
}

# Buffer settings
BUFFER_FLUSH_INTERVAL = 5  # seconds
BUFFER_FLUSH_SIZE = 1000   # records per table
DB_CHECKPOINT_INTERVAL = 3600  # 1 hour


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATA BUFFERS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class DataBuffers:
    """Thread-safe buffers for each table with exact column schemas."""
    
    prices: List[Tuple] = field(default_factory=list)
    orderbooks: List[Tuple] = field(default_factory=list)
    trades: List[Tuple] = field(default_factory=list)
    mark_prices: List[Tuple] = field(default_factory=list)
    funding_rates: List[Tuple] = field(default_factory=list)
    open_interest: List[Tuple] = field(default_factory=list)
    ticker_24h: List[Tuple] = field(default_factory=list)
    candles: List[Tuple] = field(default_factory=list)
    liquidations: List[Tuple] = field(default_factory=list)
    
    # Counters
    _id_counters: Dict[str, int] = field(default_factory=lambda: defaultdict(int))
    _lock: asyncio.Lock = field(default_factory=asyncio.Lock)
    
    def get_next_id(self, table: str) -> int:
        """Get next sequential ID for a table."""
        self._id_counters[table] += 1
        return self._id_counters[table]
    
    def total_records(self) -> int:
        """Total buffered records."""
        return (len(self.prices) + len(self.orderbooks) + len(self.trades) +
                len(self.mark_prices) + len(self.funding_rates) + len(self.open_interest) +
                len(self.ticker_24h) + len(self.candles) + len(self.liquidations))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATA COLLECTOR CLASS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ProductionDataCollector:
    """
    Production-grade data collector with exact column schemas per stream type.
    """
    
    def __init__(self, db_path: str = "data/exchange_data.duckdb"):
        self.db_path = db_path
        self.conn = None
        self.buffers = DataBuffers()
        self.running = False
        self.stats = {
            'total_records': 0,
            'records_by_table': defaultdict(int),
            'records_by_exchange': defaultdict(int),
            'records_by_market_type': defaultdict(int),
            'last_flush': datetime.now(timezone.utc),
            'start_time': None,
            'errors': 0
        }
        
    def connect(self):
        """Connect to DuckDB."""
        Path(self.db_path).parent.mkdir(parents=True, exist_ok=True)
        self.conn = duckdb.connect(self.db_path)
        
        # Load current max IDs
        tables = ['prices', 'orderbooks', 'trades', 'mark_prices', 
                  'funding_rates', 'open_interest', 'ticker_24h', 'candles', 'liquidations']
        for table in tables:
            try:
                max_id = self.conn.execute(f"SELECT MAX(id) FROM {table}").fetchone()[0]
                if max_id:
                    self.buffers._id_counters[table] = max_id
            except:
                pass
                
        logger.info(f"âœ… Connected to database: {self.db_path}")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ADD METHODS - Each with EXACT column schema
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def add_price(self, symbol: str, exchange: str, market_type: str, data: dict):
        """
        Add price tick to buffer.
        
        Schema:
        - id, timestamp, symbol, exchange, market_type, data_source
        - mid_price, bid_price, ask_price, spread, spread_bps
        """
        async with self.buffers._lock:
            record_id = self.buffers.get_next_id('prices')
            data_source = f"{exchange}_{market_type}"
            
            # Calculate values
            bid = data.get('bid')
            ask = data.get('ask')
            mid = data.get('mid_price') or data.get('price')
            
            if bid and ask and not mid:
                mid = (bid + ask) / 2
            
            spread = (ask - bid) if (bid and ask) else None
            spread_bps = (spread / mid * 10000) if (spread and mid) else None
            
            record = (
                record_id,
                datetime.now(timezone.utc),
                symbol,
                exchange,
                market_type,
                data_source,
                mid,
                bid,
                ask,
                spread,
                spread_bps
            )
            
            self.buffers.prices.append(record)
            self._update_stats('prices', exchange, market_type)
    
    async def add_orderbook(self, symbol: str, exchange: str, market_type: str, data: dict):
        """
        Add orderbook snapshot to buffer.
        
        Schema:
        - id, timestamp, symbol, exchange, market_type, data_source
        - bid_1_price...bid_10_price, bid_1_qty...bid_10_qty
        - ask_1_price...ask_10_price, ask_1_qty...ask_10_qty
        - total_bid_depth, total_ask_depth, bid_ask_ratio, spread, spread_pct, mid_price
        """
        async with self.buffers._lock:
            record_id = self.buffers.get_next_id('orderbooks')
            data_source = f"{exchange}_{market_type}"
            
            bids = data.get('bids', [])[:10]
            asks = data.get('asks', [])[:10]
            
            # Pad to 10 levels
            while len(bids) < 10:
                bids.append([None, None])
            while len(asks) < 10:
                asks.append([None, None])
            
            # Flatten bid/ask arrays
            bid_prices = [b[0] for b in bids]
            bid_qtys = [b[1] for b in bids]
            ask_prices = [a[0] for a in asks]
            ask_qtys = [a[1] for a in asks]
            
            # Aggregates
            total_bid = sum(q for q in bid_qtys if q) if bid_qtys else None
            total_ask = sum(q for q in ask_qtys if q) if ask_qtys else None
            ratio = (total_bid / total_ask) if (total_bid and total_ask) else None
            
            bid_1 = bid_prices[0]
            ask_1 = ask_prices[0]
            mid = ((bid_1 + ask_1) / 2) if (bid_1 and ask_1) else None
            spread = (ask_1 - bid_1) if (bid_1 and ask_1) else None
            spread_pct = (spread / mid * 100) if (spread and mid) else None
            
            record = (
                record_id,
                datetime.now(timezone.utc),
                symbol,
                exchange,
                market_type,
                data_source,
                # Bids (10 levels Ã— 2 = 20 values)
                *bid_prices, *bid_qtys,
                # Asks (10 levels Ã— 2 = 20 values)
                *ask_prices, *ask_qtys,
                # Aggregates (6 values)
                total_bid, total_ask, ratio, spread, spread_pct, mid
            )
            
            self.buffers.orderbooks.append(record)
            self._update_stats('orderbooks', exchange, market_type)
    
    async def add_trade(self, symbol: str, exchange: str, market_type: str, data: dict):
        """
        Add trade execution to buffer.
        
        Schema:
        - id, timestamp, symbol, exchange, market_type, data_source
        - trade_id, price, quantity, quote_value, side, is_buyer_maker
        """
        async with self.buffers._lock:
            record_id = self.buffers.get_next_id('trades')
            data_source = f"{exchange}_{market_type}"
            
            price = data.get('price')
            qty = data.get('quantity') or data.get('qty') or data.get('size')
            quote_value = (price * qty) if (price and qty) else None
            
            record = (
                record_id,
                datetime.now(timezone.utc),
                symbol,
                exchange,
                market_type,
                data_source,
                data.get('trade_id') or data.get('id'),
                price,
                qty,
                quote_value,
                data.get('side', 'unknown'),
                data.get('is_buyer_maker')
            )
            
            self.buffers.trades.append(record)
            self._update_stats('trades', exchange, market_type)
    
    async def add_mark_price(self, symbol: str, exchange: str, data: dict):
        """
        Add mark price to buffer. FUTURES ONLY.
        
        Schema:
        - id, timestamp, symbol, exchange, market_type, data_source
        - mark_price, index_price, basis, basis_pct, funding_rate, annualized_rate
        """
        async with self.buffers._lock:
            record_id = self.buffers.get_next_id('mark_prices')
            market_type = 'futures'
            data_source = f"{exchange}_{market_type}"
            
            mark = data.get('mark_price')
            index = data.get('index_price')
            funding = data.get('funding_rate')
            
            basis = (mark - index) if (mark and index) else None
            basis_pct = (basis / index * 100) if (basis and index) else None
            annualized = (funding * 3 * 365 * 100) if funding else None
            
            record = (
                record_id,
                datetime.now(timezone.utc),
                symbol,
                exchange,
                market_type,
                data_source,
                mark,
                index,
                basis,
                basis_pct,
                funding,
                annualized
            )
            
            self.buffers.mark_prices.append(record)
            self._update_stats('mark_prices', exchange, market_type)
    
    async def add_funding_rate(self, symbol: str, exchange: str, data: dict):
        """
        Add funding rate to buffer. FUTURES ONLY.
        
        Schema:
        - id, timestamp, symbol, exchange, market_type, data_source
        - funding_rate, funding_pct, annualized_pct, next_funding_time, countdown_secs
        """
        async with self.buffers._lock:
            record_id = self.buffers.get_next_id('funding_rates')
            market_type = 'futures'
            data_source = f"{exchange}_{market_type}"
            
            rate = data.get('funding_rate')
            funding_pct = (rate * 100) if rate else None
            annualized = (rate * 3 * 365 * 100) if rate else None
            
            next_time = data.get('next_funding_time')
            if next_time and isinstance(next_time, (int, float)):
                next_time = datetime.fromtimestamp(next_time / 1000, tz=timezone.utc)
            countdown = data.get('countdown_secs')
            
            record = (
                record_id,
                datetime.now(timezone.utc),
                symbol,
                exchange,
                market_type,
                data_source,
                rate,
                funding_pct,
                annualized,
                next_time,
                countdown
            )
            
            self.buffers.funding_rates.append(record)
            self._update_stats('funding_rates', exchange, market_type)
    
    async def add_open_interest(self, symbol: str, exchange: str, data: dict):
        """
        Add open interest to buffer. FUTURES ONLY.
        
        Schema:
        - id, timestamp, symbol, exchange, market_type, data_source
        - open_interest, open_interest_usd, oi_change_1h, oi_change_pct_1h
        """
        async with self.buffers._lock:
            record_id = self.buffers.get_next_id('open_interest')
            market_type = 'futures'
            data_source = f"{exchange}_{market_type}"
            
            oi = data.get('open_interest') or data.get('oi')
            oi_usd = data.get('open_interest_usd') or data.get('oi_value')
            
            record = (
                record_id,
                datetime.now(timezone.utc),
                symbol,
                exchange,
                market_type,
                data_source,
                oi,
                oi_usd,
                None,  # oi_change_1h - calculated later
                None   # oi_change_pct_1h - calculated later
            )
            
            self.buffers.open_interest.append(record)
            self._update_stats('open_interest', exchange, market_type)
    
    async def add_ticker_24h(self, symbol: str, exchange: str, market_type: str, data: dict):
        """
        Add 24h ticker to buffer. Futures and Spot (NOT oracle, NOT Hyperliquid).
        
        Schema:
        - id, timestamp, symbol, exchange, market_type, data_source
        - volume_24h, quote_volume_24h, trade_count_24h
        - high_24h, low_24h, open_24h, last_price, price_change, price_change_pct, vwap_24h
        """
        async with self.buffers._lock:
            record_id = self.buffers.get_next_id('ticker_24h')
            data_source = f"{exchange}_{market_type}"
            
            vol = data.get('volume_24h') or data.get('volume')
            quote_vol = data.get('quote_volume_24h') or data.get('turnover') or data.get('quoteVolume')
            vwap = (quote_vol / vol) if (vol and quote_vol and vol > 0) else None
            
            record = (
                record_id,
                datetime.now(timezone.utc),
                symbol,
                exchange,
                market_type,
                data_source,
                vol,
                quote_vol,
                data.get('trade_count_24h') or data.get('count'),
                data.get('high_24h') or data.get('high'),
                data.get('low_24h') or data.get('low'),
                data.get('open_24h') or data.get('open'),
                data.get('last_price') or data.get('lastPrice') or data.get('close'),
                data.get('price_change') or data.get('priceChange'),
                data.get('price_change_pct') or data.get('priceChangePercent'),
                vwap
            )
            
            self.buffers.ticker_24h.append(record)
            self._update_stats('ticker_24h', exchange, market_type)
    
    async def add_candle(self, symbol: str, exchange: str, market_type: str, data: dict):
        """
        Add OHLCV candle to buffer. Futures and Spot (NOT oracle).
        
        Schema:
        - id, open_time, close_time, symbol, exchange, market_type, data_source, interval
        - open, high, low, close, volume, quote_volume
        - trade_count, taker_buy_volume, taker_buy_quote, taker_buy_pct
        """
        async with self.buffers._lock:
            record_id = self.buffers.get_next_id('candles')
            data_source = f"{exchange}_{market_type}"
            
            open_time = data.get('open_time') or data.get('start')
            if isinstance(open_time, (int, float)):
                open_time = datetime.fromtimestamp(open_time / 1000, tz=timezone.utc)
            
            close_time = data.get('close_time') or data.get('end')
            if isinstance(close_time, (int, float)):
                close_time = datetime.fromtimestamp(close_time / 1000, tz=timezone.utc)
            
            vol = data.get('volume')
            taker_buy = data.get('taker_buy_volume')
            taker_pct = (taker_buy / vol * 100) if (vol and taker_buy and vol > 0) else None
            
            record = (
                record_id,
                open_time,
                close_time,
                symbol,
                exchange,
                market_type,
                data_source,
                data.get('interval', '1m'),
                data.get('open'),
                data.get('high'),
                data.get('low'),
                data.get('close'),
                vol,
                data.get('quote_volume') or data.get('turnover'),
                data.get('trade_count'),
                taker_buy,
                data.get('taker_buy_quote'),
                taker_pct
            )
            
            self.buffers.candles.append(record)
            self._update_stats('candles', exchange, market_type)
    
    async def add_liquidation(self, symbol: str, exchange: str, data: dict):
        """
        Add liquidation to buffer. FUTURES ONLY (NOT Kraken).
        
        Schema:
        - id, timestamp, symbol, exchange, market_type, data_source
        - side, price, quantity, value_usd, is_large
        """
        async with self.buffers._lock:
            record_id = self.buffers.get_next_id('liquidations')
            market_type = 'futures'
            data_source = f"{exchange}_{market_type}"
            
            price = data.get('price')
            qty = data.get('quantity') or data.get('size')
            value = (price * qty) if (price and qty) else None
            is_large = (value > 100000) if value else False
            
            record = (
                record_id,
                datetime.now(timezone.utc),
                symbol,
                exchange,
                market_type,
                data_source,
                data.get('side', 'unknown'),
                price,
                qty,
                value,
                is_large
            )
            
            self.buffers.liquidations.append(record)
            self._update_stats('liquidations', exchange, market_type)
    
    def _update_stats(self, table: str, exchange: str, market_type: str):
        """Update collection statistics."""
        self.stats['total_records'] += 1
        self.stats['records_by_table'][table] += 1
        self.stats['records_by_exchange'][exchange] += 1
        self.stats['records_by_market_type'][market_type] += 1
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # FLUSH TO DATABASE
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def flush_buffers(self):
        """Flush all buffers to DuckDB with exact column mappings."""
        async with self.buffers._lock:
            start_time = time.time()
            flushed = 0
            
            # PRICES (11 columns)
            if self.buffers.prices:
                self.conn.executemany("""
                    INSERT INTO prices (id, timestamp, symbol, exchange, market_type, data_source,
                                       mid_price, bid_price, ask_price, spread, spread_bps)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, self.buffers.prices)
                flushed += len(self.buffers.prices)
                self.buffers.prices = []
            
            # ORDERBOOKS (46 columns)
            if self.buffers.orderbooks:
                self.conn.executemany("""
                    INSERT INTO orderbooks (
                        id, timestamp, symbol, exchange, market_type, data_source,
                        bid_1_price, bid_2_price, bid_3_price, bid_4_price, bid_5_price,
                        bid_6_price, bid_7_price, bid_8_price, bid_9_price, bid_10_price,
                        bid_1_qty, bid_2_qty, bid_3_qty, bid_4_qty, bid_5_qty,
                        bid_6_qty, bid_7_qty, bid_8_qty, bid_9_qty, bid_10_qty,
                        ask_1_price, ask_2_price, ask_3_price, ask_4_price, ask_5_price,
                        ask_6_price, ask_7_price, ask_8_price, ask_9_price, ask_10_price,
                        ask_1_qty, ask_2_qty, ask_3_qty, ask_4_qty, ask_5_qty,
                        ask_6_qty, ask_7_qty, ask_8_qty, ask_9_qty, ask_10_qty,
                        total_bid_depth, total_ask_depth, bid_ask_ratio, spread, spread_pct, mid_price
                    )
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,
                            ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,
                            ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, self.buffers.orderbooks)
                flushed += len(self.buffers.orderbooks)
                self.buffers.orderbooks = []
            
            # TRADES (12 columns)
            if self.buffers.trades:
                self.conn.executemany("""
                    INSERT INTO trades (id, timestamp, symbol, exchange, market_type, data_source,
                                       trade_id, price, quantity, quote_value, side, is_buyer_maker)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, self.buffers.trades)
                flushed += len(self.buffers.trades)
                self.buffers.trades = []
            
            # MARK PRICES (12 columns)
            if self.buffers.mark_prices:
                self.conn.executemany("""
                    INSERT INTO mark_prices (id, timestamp, symbol, exchange, market_type, data_source,
                                            mark_price, index_price, basis, basis_pct, funding_rate, annualized_rate)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, self.buffers.mark_prices)
                flushed += len(self.buffers.mark_prices)
                self.buffers.mark_prices = []
            
            # FUNDING RATES (11 columns)
            if self.buffers.funding_rates:
                self.conn.executemany("""
                    INSERT INTO funding_rates (id, timestamp, symbol, exchange, market_type, data_source,
                                              funding_rate, funding_pct, annualized_pct, next_funding_time, countdown_secs)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, self.buffers.funding_rates)
                flushed += len(self.buffers.funding_rates)
                self.buffers.funding_rates = []
            
            # OPEN INTEREST (10 columns)
            if self.buffers.open_interest:
                self.conn.executemany("""
                    INSERT INTO open_interest (id, timestamp, symbol, exchange, market_type, data_source,
                                              open_interest, open_interest_usd, oi_change_1h, oi_change_pct_1h)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, self.buffers.open_interest)
                flushed += len(self.buffers.open_interest)
                self.buffers.open_interest = []
            
            # TICKER 24H (16 columns)
            if self.buffers.ticker_24h:
                self.conn.executemany("""
                    INSERT INTO ticker_24h (id, timestamp, symbol, exchange, market_type, data_source,
                                           volume_24h, quote_volume_24h, trade_count_24h,
                                           high_24h, low_24h, open_24h, last_price,
                                           price_change, price_change_pct, vwap_24h)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, self.buffers.ticker_24h)
                flushed += len(self.buffers.ticker_24h)
                self.buffers.ticker_24h = []
            
            # CANDLES (18 columns)
            if self.buffers.candles:
                self.conn.executemany("""
                    INSERT INTO candles (id, open_time, close_time, symbol, exchange, market_type, data_source, interval,
                                        open, high, low, close, volume, quote_volume,
                                        trade_count, taker_buy_volume, taker_buy_quote, taker_buy_pct)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, self.buffers.candles)
                flushed += len(self.buffers.candles)
                self.buffers.candles = []
            
            # LIQUIDATIONS (11 columns)
            if self.buffers.liquidations:
                self.conn.executemany("""
                    INSERT INTO liquidations (id, timestamp, symbol, exchange, market_type, data_source,
                                             side, price, quantity, value_usd, is_large)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, self.buffers.liquidations)
                flushed += len(self.buffers.liquidations)
                self.buffers.liquidations = []
            
            duration_ms = (time.time() - start_time) * 1000
            if flushed > 0:
                logger.info(f"ğŸ’¾ Flushed {flushed:,} records in {duration_ms:.1f}ms")
            
            self.stats['last_flush'] = datetime.now(timezone.utc)
            return flushed
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # BACKGROUND TASKS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def periodic_flush(self):
        """Flush buffers periodically."""
        while self.running:
            await asyncio.sleep(BUFFER_FLUSH_INTERVAL)
            try:
                await self.flush_buffers()
            except Exception as e:
                logger.error(f"Flush error: {e}")
                self.stats['errors'] += 1
    
    async def periodic_checkpoint(self):
        """Create database checkpoint hourly."""
        while self.running:
            await asyncio.sleep(DB_CHECKPOINT_INTERVAL)
            try:
                self.conn.execute("CHECKPOINT")
                db_size = os.path.getsize(self.db_path)
                logger.info(f"ğŸ”„ Database checkpoint - Size: {db_size / (1024*1024):.2f} MB")
            except Exception as e:
                logger.error(f"Checkpoint error: {e}")
    
    async def log_stats(self):
        """Log collection statistics every minute."""
        while self.running:
            await asyncio.sleep(60)
            elapsed = (datetime.now(timezone.utc) - self.stats['start_time']).total_seconds()
            rate = self.stats['total_records'] / elapsed if elapsed > 0 else 0
            
            logger.info(f"""
ğŸ“Š COLLECTION STATS ({elapsed/60:.1f} minutes)
   Total Records: {self.stats['total_records']:,}
   Rate: {rate:.1f}/sec
   By Market Type: {dict(self.stats['records_by_market_type'])}
   Errors: {self.stats['errors']}
""")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MAIN RUN
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def run(self):
        """Main entry point for data collection."""
        self.connect()
        self.running = True
        self.stats['start_time'] = datetime.now(timezone.utc)
        
        logger.info("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸš€ STARTING 24/7 DATA COLLECTION                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Symbols: 9 (5 major + 4 meme)                                               â•‘
â•‘  Exchanges: 9 (6 futures + 2 spot + 1 oracle)                                â•‘
â•‘  Data Feeds: 527                                                             â•‘
â•‘  Market Types: futures, spot, oracle (STRICTLY SEPARATED)                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
        
        # Start background tasks
        tasks = [
            asyncio.create_task(self.periodic_flush()),
            asyncio.create_task(self.periodic_checkpoint()),
            asyncio.create_task(self.log_stats()),
        ]
        
        # NOTE: Real WebSocket connections would be added here
        # For now, this is the framework - you need to import direct_exchange_client
        # and connect the actual WebSocket handlers
        
        logger.info("âš¡ Background tasks started")
        logger.info("ğŸ“¡ Waiting for WebSocket connections to be implemented...")
        
        try:
            await asyncio.gather(*tasks)
        except asyncio.CancelledError:
            logger.info("Shutting down...")
        finally:
            await self.flush_buffers()
            self.conn.close()
            self.running = False


def print_table_schemas():
    """Print all table schemas for reference."""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                               TABLE SCHEMAS WITH EXACT COLUMNS                                    â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£

ğŸ“Š PRICES (11 columns) - All Market Types
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ id, timestamp, symbol, exchange, market_type, data_source,                  â”‚
â”‚ mid_price, bid_price, ask_price, spread, spread_bps                         â”‚
â”‚                                                                             â”‚
â”‚ Note: Oracle has NULL bid_price, ask_price, spread, spread_bps             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š ORDERBOOKS (52 columns) - Futures & Spot Only
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ id, timestamp, symbol, exchange, market_type, data_source,                  â”‚
â”‚ bid_1_price...bid_10_price (10), bid_1_qty...bid_10_qty (10),              â”‚
â”‚ ask_1_price...ask_10_price (10), ask_1_qty...ask_10_qty (10),              â”‚
â”‚ total_bid_depth, total_ask_depth, bid_ask_ratio, spread, spread_pct,       â”‚
â”‚ mid_price                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š TRADES (12 columns) - Futures & Spot Only
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ id, timestamp, symbol, exchange, market_type, data_source,                  â”‚
â”‚ trade_id, price, quantity, quote_value, side, is_buyer_maker               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š MARK_PRICES (12 columns) - FUTURES ONLY
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ id, timestamp, symbol, exchange, market_type, data_source,                  â”‚
â”‚ mark_price, index_price, basis, basis_pct, funding_rate, annualized_rate   â”‚
â”‚                                                                             â”‚
â”‚ Note: index_price NULL for OKX, Kraken, Hyperliquid                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š FUNDING_RATES (11 columns) - FUTURES ONLY
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ id, timestamp, symbol, exchange, market_type, data_source,                  â”‚
â”‚ funding_rate, funding_pct, annualized_pct, next_funding_time, countdown_secsâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š OPEN_INTEREST (10 columns) - FUTURES ONLY
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ id, timestamp, symbol, exchange, market_type, data_source,                  â”‚
â”‚ open_interest, open_interest_usd, oi_change_1h, oi_change_pct_1h           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š TICKER_24H (16 columns) - Futures & Spot (NOT Hyperliquid)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ id, timestamp, symbol, exchange, market_type, data_source,                  â”‚
â”‚ volume_24h, quote_volume_24h, trade_count_24h,                             â”‚
â”‚ high_24h, low_24h, open_24h, last_price, price_change, price_change_pct,   â”‚
â”‚ vwap_24h                                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š CANDLES (18 columns) - Futures & Spot Only
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ id, open_time, close_time, symbol, exchange, market_type, data_source,     â”‚
â”‚ interval, open, high, low, close, volume, quote_volume,                    â”‚
â”‚ trade_count, taker_buy_volume, taker_buy_quote, taker_buy_pct              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š LIQUIDATIONS (11 columns) - FUTURES ONLY (NOT Kraken)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ id, timestamp, symbol, exchange, market_type, data_source,                  â”‚
â”‚ side, price, quantity, value_usd, is_large                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                     MARKET TYPE AVAILABILITY                                      â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â”‚ Stream          â”‚ futures â”‚ spot â”‚ oracle â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ prices          â”‚    âœ…   â”‚  âœ…  â”‚   âœ…   â”‚
â”‚ orderbooks      â”‚    âœ…   â”‚  âœ…  â”‚   âŒ   â”‚
â”‚ trades          â”‚    âœ…   â”‚  âœ…  â”‚   âŒ   â”‚
â”‚ mark_prices     â”‚    âœ…   â”‚  âŒ  â”‚   âŒ   â”‚
â”‚ funding_rates   â”‚    âœ…   â”‚  âŒ  â”‚   âŒ   â”‚
â”‚ open_interest   â”‚    âœ…   â”‚  âŒ  â”‚   âŒ   â”‚
â”‚ ticker_24h      â”‚    âœ…*  â”‚  âœ…  â”‚   âŒ   â”‚
â”‚ candles         â”‚    âœ…   â”‚  âœ…  â”‚   âŒ   â”‚
â”‚ liquidations    â”‚    âœ…** â”‚  âŒ  â”‚   âŒ   â”‚
â”‚                                            â”‚
â”‚ * Not available on Hyperliquid             â”‚
â”‚ ** Not available on Kraken                 â”‚
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")


if __name__ == "__main__":
    print_table_schemas()
    
    # Create logs directory
    Path("logs").mkdir(exist_ok=True)
    
    # Run collector
    collector = ProductionDataCollector()
    asyncio.run(collector.run())
